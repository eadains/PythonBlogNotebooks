{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sparse_linear(n_samples=100, n_features=20, n_nonzero=5, noise_std=0.1):\n",
    "    \"\"\"\n",
    "    Generate data from a sparse linear model with Gaussian noise.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of samples\n",
    "    n_features : int\n",
    "        Number of features\n",
    "    n_nonzero : int\n",
    "        Number of nonzero coefficients\n",
    "    noise_std : float\n",
    "        Standard deviation of Gaussian noise\n",
    "    random_state : int\n",
    "        Random seed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : array, shape (n_samples, n_features)\n",
    "        Feature matrix\n",
    "    y : array, shape (n_samples,)\n",
    "        Target vector\n",
    "    beta : array, shape (n_features,)\n",
    "        True coefficients\n",
    "    \"\"\"\n",
    "    # Generate random feature matrix\n",
    "    X = rng.standard_normal((n_samples, n_features))\n",
    "\n",
    "    # Generate sparse coefficients\n",
    "    beta = np.zeros(n_features)\n",
    "    nonzero_idx = rng.choice(n_features, size=n_nonzero, replace=False)\n",
    "    beta[nonzero_idx] = rng.standard_normal(n_nonzero)\n",
    "\n",
    "    # Generate target with noise\n",
    "    y = X @ beta + noise_std * rng.standard_normal(n_samples)\n",
    "\n",
    "    return X, y, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, beta_true = sample_sparse_linear()\n",
    "# Adding interecept column to X\n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(y, y_hat):\n",
    "    return np.sum(np.square(y - y_hat))\n",
    "\n",
    "\n",
    "def loss_func_deriv(X, beta, y):\n",
    "    return -2 * X.T @ (y - X @ beta)\n",
    "\n",
    "\n",
    "def check_obj_stop(f_old, f_new, eps):\n",
    "    return np.abs(f_new - f_old) < eps * np.abs(f_old)\n",
    "\n",
    "\n",
    "def check_params_stop(params_old, params_new, eps):\n",
    "    return np.linalg.norm(params_new - params_old) < eps * np.linalg.norm(params_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676.7973046204713\n",
      "342.2761547552429\n",
      "126.67538572635736\n",
      "67.68384942477226\n",
      "46.68496333554782\n",
      "22.516704972150098\n",
      "14.57815409776227\n",
      "11.298869967455445\n",
      "6.077860016132026\n",
      "5.504937192383445\n",
      "2.64817067588246\n",
      "1.7913813795609295\n",
      "1.3929655982258204\n",
      "0.7715146377771303\n",
      "0.6757503573761724\n",
      "0.33715337753919444\n",
      "0.23181681081578018\n",
      "0.1741392800699846\n",
      "0.09981944389793353\n",
      "0.08384535525339581\n",
      "0.04345585701086835\n",
      "0.0411161399521352\n"
     ]
    }
   ],
   "source": [
    "beta = np.zeros(X.shape[1])\n",
    "eps_params = 1e-5\n",
    "eps_obj = 1e-5\n",
    "\n",
    "while True:\n",
    "    del_f = loss_func_deriv(X, beta, y)\n",
    "    print(np.linalg.norm(del_f))\n",
    "    p = -del_f\n",
    "\n",
    "    if np.linalg.norm(del_f) < 1e-5:\n",
    "        break\n",
    "\n",
    "    def find_step_size(alpha=1, c=0.5, tau=0.5):\n",
    "        # Backtracking line search\n",
    "        # https://en.wikipedia.org/wiki/Backtracking_line_search\n",
    "        m = loss_func_deriv(X, beta, y).T @ p\n",
    "        t = -c * m\n",
    "        while loss_func(X @ beta, y) - loss_func(X @ (beta + alpha * p), y) < alpha * t:\n",
    "            alpha = tau * alpha\n",
    "        return alpha\n",
    "\n",
    "    beta_new = beta + find_step_size() * p\n",
    "    if check_obj_stop(\n",
    "        loss_func(y, X @ beta), loss_func(y, X @ beta_new), eps_obj\n",
    "    ) or check_params_stop(beta, beta_new, eps_params):\n",
    "        break\n",
    "\n",
    "    beta = beta_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02,  0.01,  0.03, -2.08, -0.01,  0.02, -0.  ,  0.03,  0.  ,\n",
       "        0.  ,  0.  , -0.  ,  0.01, -0.16, -0.  ,  0.  , -1.6 , -1.36,\n",
       "       -1.48,  0.  ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta[1:].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.  ,  0.  ,  0.  , -2.08,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "        0.  ,  0.  ,  0.  ,  0.  , -0.16,  0.  ,  0.  , -1.6 , -1.35,\n",
       "       -1.48,  0.  ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_true.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox_op(x, lam):\n",
    "    return np.sign(x) * np.maximum((np.abs(x) - lam), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676.7973046204713\n",
      "350.0573086153344\n",
      "125.88164431583853\n",
      "70.03564780167251\n",
      "40.5362254477239\n",
      "19.18461300213021\n",
      "11.73979837729861\n",
      "11.196462590622499\n",
      "9.403728947997001\n",
      "9.012624930720934\n",
      "8.39339981092738\n",
      "8.233875299098766\n",
      "8.146457683724242\n",
      "8.113722452910492\n",
      "8.09728659271841\n",
      "8.089816512147406\n",
      "8.086074222198322\n",
      "8.084228766546477\n"
     ]
    }
   ],
   "source": [
    "beta = np.zeros(X.shape[1])\n",
    "lam = 0.01\n",
    "eps_params = 1e-5\n",
    "eps_obj = 1e-5\n",
    "\n",
    "while True:\n",
    "    del_f = loss_func_deriv(X, beta, y)\n",
    "    print(np.linalg.norm(del_f))\n",
    "    p = -del_f\n",
    "\n",
    "    def find_step_size(alpha=1, c=0.5, tau=0.5):\n",
    "        # Backtracking line search\n",
    "        # https://en.wikipedia.org/wiki/Backtracking_line_search\n",
    "        m = loss_func_deriv(X, beta, y).T @ p\n",
    "        t = -c * m\n",
    "        while loss_func(X @ beta, y) - loss_func(X @ (beta + alpha * p), y) < alpha * t:\n",
    "            alpha = tau * alpha\n",
    "        return alpha\n",
    "\n",
    "    beta_new = prox_op(beta + find_step_size() * p, lam)\n",
    "    if check_obj_stop(\n",
    "        loss_func(y, X @ beta), loss_func(y, X @ beta_new), eps_obj\n",
    "    ) or check_params_stop(beta, beta_new, eps_params):\n",
    "        break\n",
    "\n",
    "    beta = beta_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.  , -0.  ,  0.01, -2.06,  0.  ,  0.  , -0.  ,  0.  ,  0.  ,\n",
       "        0.  , -0.  ,  0.  , -0.  , -0.15,  0.  ,  0.  , -1.59, -1.35,\n",
       "       -1.47, -0.  ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta[1:].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.  ,  0.  ,  0.  , -2.08,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "        0.  ,  0.  ,  0.  ,  0.  , -0.16,  0.  ,  0.  , -1.6 , -1.35,\n",
       "       -1.48,  0.  ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_true.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sparse_gamma(n_samples=100, n_features=20, n_nonzero=5, noise_scale=0.1):\n",
    "    \"\"\"\n",
    "    Generate data from a sparse linear model with Gamma noise.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of samples\n",
    "    n_features : int\n",
    "        Number of features\n",
    "    n_nonzero : int\n",
    "        Number of nonzero coefficients\n",
    "    noise_scale : float\n",
    "        Scale parameter for Gamma noise\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : array, shape (n_samples, n_features)\n",
    "        Feature matrix\n",
    "    y : array, shape (n_samples,)\n",
    "        Target vector\n",
    "    beta : array, shape (n_features,)\n",
    "        True coefficients\n",
    "    \"\"\"\n",
    "    # Generate random feature matrix\n",
    "    # X = rng.standard_normal((n_samples, n_features))\n",
    "    X = rng.uniform(0.1, 0.5, (n_samples, n_features))\n",
    "\n",
    "    # Generate sparse coefficients\n",
    "    beta = np.zeros(n_features)\n",
    "    nonzero_idx = rng.choice(n_features, size=n_nonzero, replace=False)\n",
    "    beta[nonzero_idx] = rng.standard_normal(n_nonzero)\n",
    "\n",
    "    # Generate target with gamma noise\n",
    "    eta = X @ beta\n",
    "    mu = np.exp(eta)  # Log link\n",
    "    shape = 1 / noise_scale\n",
    "    scale = mu / shape  # Set scale so mean = mu\n",
    "    y = rng.gamma(shape, scale)\n",
    "\n",
    "    return X, y, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, beta_true = sample_sparse_gamma(n_samples=100000, n_features=10, noise_scale=1)\n",
    "# Adding interecept column to X\n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(y_hat, y):\n",
    "    # y_hat = jnp.exp(y_hat)\n",
    "    # return 2 * jnp.sum(-jnp.log(y / y_hat) + (y - y_hat) / y_hat)\n",
    "    return 2 * jnp.sum(y_hat + y * jnp.exp(-y_hat))\n",
    "\n",
    "\n",
    "def gen_grad(x, del_g, prox_op, t):\n",
    "    return (x - prox_op(x - t * del_g)) / t\n",
    "\n",
    "\n",
    "loss_func_deriv = jax.grad(lambda beta: loss_func(X @ beta, y))\n",
    "loss_func_hess = jax.hessian(lambda beta: loss_func(X @ beta, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 0.8\n",
      "Param diff: 4.135533960814055\n",
      "Step size: 1\n",
      "Param diff: 1.1878580670121504\n",
      "Step size: 1\n",
      "Param diff: 1.0813510405434958\n",
      "Step size: 1\n",
      "Param diff: 0.2843124918808631\n",
      "Step size: 3.484491437270418e-05\n",
      "Param diff: 0.026525890804703668\n",
      "Step size: 1\n",
      "Param diff: 0.021149545894374375\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.02658519363112942\n",
      "Step size: 1\n",
      "Param diff: 0.02677324559020136\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.02658854918318828\n",
      "Step size: 1\n",
      "Param diff: 0.02658305349929286\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.026588532891142564\n",
      "Step size: 1\n",
      "Param diff: 0.026588149585396438\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.026588410800489858\n",
      "Step size: 1\n",
      "Param diff: 0.026589176383429194\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.026588489857099863\n",
      "Step size: 1\n",
      "Param diff: 0.02658847172300747\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.02658849341610533\n",
      "Step size: 1\n",
      "Param diff: 0.02658769732153098\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.02658852041186351\n",
      "Step size: 1\n",
      "Param diff: 0.026588444423502274\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.026588466789772364\n",
      "Step size: 1\n",
      "Param diff: 0.02658933612741341\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.026588499677118262\n",
      "Step size: 1\n",
      "Param diff: 0.0265883320986028\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.02658849951946298\n",
      "Step size: 1\n",
      "Param diff: 0.026588400692805388\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.026588515576748684\n",
      "Step size: 1\n",
      "Param diff: 0.026587615480058484\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.026588468010751574\n",
      "Step size: 1\n",
      "Param diff: 0.026589650345937594\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.026588505679297488\n",
      "Step size: 1\n",
      "Param diff: 0.026588199476411518\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.026588502096690912\n",
      "Step size: 1\n",
      "Param diff: 0.026588199816915726\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.026588503137888513\n",
      "Step size: 1\n",
      "Param diff: 0.02658899299184421\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.026588478801814305\n",
      "Step size: 1\n",
      "Param diff: 0.026588839528995385\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.02658849974807693\n",
      "Step size: 1\n",
      "Param diff: 0.026587401605821494\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.026588473402499743\n",
      "Step size: 1\n",
      "Param diff: 0.0265885594592387\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.02658848097328557\n",
      "Step size: 1\n",
      "Param diff: 0.02658905645237217\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.026588506516010177\n",
      "Step size: 1\n",
      "Param diff: 0.026588073082141228\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.026588477977535357\n",
      "Step size: 1\n",
      "Param diff: 0.02658870938679342\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.026588483807920768\n",
      "Step size: 1\n",
      "Param diff: 0.026588850876458167\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.026588478886317803\n",
      "Step size: 1\n",
      "Param diff: 0.026587837352800212\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.0265884783208155\n",
      "Step size: 1\n",
      "Param diff: 0.026589099120611765\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.026588449022300512\n",
      "Step size: 1\n",
      "Param diff: 0.026588349101116134\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.026588457680854997\n",
      "Step size: 1\n",
      "Param diff: 0.026588617331203403\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.02658847651958259\n",
      "Step size: 1\n",
      "Param diff: 0.026588377219217023\n",
      "Step size: 2.7875931498163346e-05\n",
      "Param diff: 0.02658849559342995\n",
      "Step size: 1\n",
      "Param diff: 0.02658808904537156\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[170], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m---> 30\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mfind_step_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m beta_new \u001b[38;5;241m=\u001b[39m beta \u001b[38;5;241m-\u001b[39m t \u001b[38;5;241m*\u001b[39m gen_grad(beta, del_g, \u001b[38;5;28;01mlambda\u001b[39;00m x: prox_op(x, lam), t)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParam diff: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(beta_new\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mbeta)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[170], line 17\u001b[0m, in \u001b[0;36mfind_step_size\u001b[0;34m(t, c, tau, max_iter)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_step_size\u001b[39m(t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Backtracking line search\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# https://en.wikipedia.org/wiki/Backtracking_line_search\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     current_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[1;32m     19\u001b[0m         G \u001b[38;5;241m=\u001b[39m gen_grad(beta, del_g, \u001b[38;5;28;01mlambda\u001b[39;00m x: prox_op(x, lam), t)\n",
      "Cell \u001b[0;32mIn[148], line 4\u001b[0m, in \u001b[0;36mloss_func\u001b[0;34m(y_hat, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_func\u001b[39m(y_hat, y):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# y_hat = jnp.exp(y_hat)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# return 2 * jnp.sum(-jnp.log(y / y_hat) + (y - y_hat) / y_hat)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m jnp\u001b[38;5;241m.\u001b[39msum(y_hat \u001b[38;5;241m+\u001b[39m y \u001b[38;5;241m*\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43my_hat\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "beta = np.hstack([[1], np.zeros(X.shape[1] - 1)])\n",
    "lam = 0.01\n",
    "eps_params = 1e-5\n",
    "eps_obj = 1e-5\n",
    "\n",
    "while True:\n",
    "    grad = loss_func_deriv(beta)\n",
    "    hess = loss_func_hess(beta)\n",
    "    del_g = jnp.linalg.solve(hess, grad)\n",
    "\n",
    "    # When n_features to n_samples ratio starts to get large better results with tau=0.8 and c=0.75\n",
    "    # Converge speed seems to depend greatly on selections of line search parameters compared to that ratio\n",
    "    # TODO: Find better step size selection algorithm\n",
    "    def find_step_size(t=1, c=0.25, tau=0.8, max_iter=100):\n",
    "        # Backtracking line search\n",
    "        # https://en.wikipedia.org/wiki/Backtracking_line_search\n",
    "        current_loss = loss_func(X @ beta, y)\n",
    "        for _ in range(max_iter):\n",
    "            G = gen_grad(beta, del_g, lambda x: prox_op(x, lam), t)\n",
    "            new_loss = loss_func(X @ (beta - t * G), y)\n",
    "            if jnp.isnan(new_loss) or (\n",
    "                new_loss\n",
    "                > current_loss - t * del_g.T @ G + t / 2 * jnp.linalg.norm(G) ** 2\n",
    "            ):\n",
    "                t = tau * t\n",
    "            else:\n",
    "                break\n",
    "        print(f\"Step size: {t}\")\n",
    "        return t\n",
    "\n",
    "    t = find_step_size()\n",
    "    beta_new = beta - t * gen_grad(beta, del_g, lambda x: prox_op(x, lam), t)\n",
    "    print(f\"Param diff: {np.linalg.norm(beta_new - beta)}\")\n",
    "    if check_obj_stop(\n",
    "        loss_func(X @ beta, y), loss_func(X @ beta_new, y), eps_obj\n",
    "    ) and check_params_stop(beta, beta_new, eps_params):\n",
    "        break\n",
    "\n",
    "    beta = beta_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.  ,  0.4 ,  0.01, -0.65,  0.  ,  0.59, -0.04, -0.13, -0.  ,\n",
       "       -0.  ])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta[1:].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.05,  0.45,  0.  , -0.67,  0.  ,  0.57,  0.  , -0.14,  0.  ,\n",
       "        0.  ])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_true.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
