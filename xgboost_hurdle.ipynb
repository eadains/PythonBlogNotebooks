{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "import numpy as np\n",
    "import jax as jax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq = pl.read_csv(\"./data/insurance/freMTPL2freq.csv\")\n",
    "\n",
    "df_sev = (\n",
    "    pl.read_csv(\"./data/insurance/freMTPL2sev.csv\", infer_schema_length=25000)\n",
    "    .group_by(\"IDpol\")\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "df = df_freq.join(df_sev, on=\"IDpol\", how=\"left\", coalesce=True).with_columns(\n",
    "    PurePremium=pl.col(\"ClaimAmount\").fill_null(0) / pl.col(\"Exposure\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def log_norm_pdf(x, mu, sigma):\n",
    "    return (\n",
    "        1\n",
    "        / (x * sigma * jnp.sqrt(2 * jnp.pi))\n",
    "        * jnp.exp(-jnp.square(jnp.log(x) - mu) / (2 * jnp.square(sigma)))\n",
    "    )\n",
    "\n",
    "@jax.jit\n",
    "def hurdle_log_prob(x, params):\n",
    "    p = jsp.special.expit(params[0])\n",
    "    mu = params[1]\n",
    "    sigma = jnp.exp(params[2])\n",
    "    # Returning negative log-prob for minimization\n",
    "    return -jnp.where(\n",
    "        x == 0, jnp.log(1 - p), jnp.log(p) + jnp.log(log_norm_pdf(x, mu, sigma))\n",
    "    )\n",
    "\n",
    "hlp_jac_func = jax.jacfwd(hurdle_log_prob, 1)\n",
    "hlp_hess_func = jax.jacfwd(hlp_jac_func, 1)\n",
    "\n",
    "@jax.jit\n",
    "def hlp_grad(x, params):\n",
    "    # When observed value is zero it has no gradient information for the log-normal\n",
    "    # part of the model, so fill these parts with 0\n",
    "    return jnp.nan_to_num(hlp_jac_func(x, params), nan=0.0)\n",
    "\n",
    "@jax.jit\n",
    "def hlp_hess(x, params):\n",
    "    return jnp.nan_to_num(hlp_hess_func(x, params), nan=0.0)\n",
    "\n",
    "# Xgboost will output a vector of predictions for each input giving an nxm matrix\n",
    "# so we need to map over each parameter vector for each datapoint\n",
    "# Param vector is [n, m] and the log-prob function requires a param vector of shape [m,]\n",
    "vector_grad = jax.vmap(hlp_grad, (0, 0), 0) # [n,] x [n, m] -> [n, m]\n",
    "vector_hess = jax.vmap(hlp_hess, (0, 0), 0) # [n,] x [n, m] -> [n, m, m]\n",
    "\n",
    "@jax.jit\n",
    "def xg_obj(y_true, y_pred):\n",
    "    grad = vector_grad(y_true, y_pred)\n",
    "    hess = vector_hess(y_true, y_pred)\n",
    "    # Xgboost needs hessian in [n, m] shape\n",
    "    return grad, jnp.abs(hess).sum(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df[\"PurePremium\"].to_numpy()\n",
    "# Test predictions in same form as xgboost produces\n",
    "y_pred = rng.random((y_true.shape[0], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, test2 = xg_obj(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
