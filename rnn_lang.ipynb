{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_sequence, pad_packed_sequence\n",
    "\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(string.ascii_letters) + list(string.digits) + list(string.punctuation) + [\" \"] + [\"UNK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(vocab: list[str], input: str) -> torch.Tensor:\n",
    "    vecs = []\n",
    "    for char in input:\n",
    "        try:\n",
    "            idx = vocab.index(char)\n",
    "        except ValueError:\n",
    "            idx = -1\n",
    "        vec = torch.zeros(len(vocab))\n",
    "        vec[idx] = 1\n",
    "        vecs.append(vec)\n",
    "    return torch.stack(vecs)\n",
    "\n",
    "\n",
    "def detokenize(vocab: list[str], input: torch.Tensor) -> str:\n",
    "    output = \"\"\n",
    "    for token in input:\n",
    "        idx = torch.nonzero(token)\n",
    "        output += vocab[idx]\n",
    "    return output\n",
    "\n",
    "\n",
    "def make_training_pair(tokens, context_length):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for i in range(len(tokens) - 1):\n",
    "        if i < context_length:\n",
    "            # Add one here because the : is exclusive on the right\n",
    "            input = tokens[: i + 1]\n",
    "        else:\n",
    "            input = tokens[i - context_length : i]\n",
    "\n",
    "        # Loss function expects output class as integer and not one-hot\n",
    "        label = torch.nonzero(tokens[i + 1])\n",
    "\n",
    "        inputs.append(input)\n",
    "        labels.append(label)\n",
    "\n",
    "    return pack_sequence(inputs, enforce_sorted=False), torch.stack(labels).flatten()\n",
    "\n",
    "\n",
    "class MovieReviewDataset(Dataset):\n",
    "    def __init__(self, review_dir: str, context_length: int, vocab: list[str]) -> None:\n",
    "        self.dir = review_dir\n",
    "        self.context_length = context_length\n",
    "        self.vocab = vocab\n",
    "        self.files = [item for item in os.listdir(review_dir)]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index: int) -> str:\n",
    "        with open(self.dir + self.files[index], \"r\") as file:\n",
    "            string = file.read()\n",
    "        tokens = tokenize(self.vocab, string)\n",
    "        return make_training_pair(tokens, self.context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MovieReviewDataset(\n",
    "    \"./data/aclImdb/train/unsup/\", context_length=128, vocab=vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieModel(nn.Module):\n",
    "    def __init__(self, vocab_length, hidden_size, num_layers) -> None:\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(vocab_length, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_length)\n",
    "        self.sm = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out = self.lstm(x)[0]\n",
    "        # Output of LSTM is PackedSequence, so turn that into a tensor of padded sequences\n",
    "        lstm_out, lengths = pad_packed_sequence(lstm_out)\n",
    "        # By using lengths of the sequences, select the last output of the LSTM\n",
    "        lstm_out = lstm_out[lengths - 1, torch.arange(0, len(lengths)), :]\n",
    "        return self.sm(self.linear(lstm_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "model = MovieModel(len(vocab), 512, 3)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=0.1)\n",
    "loss_func = nn.NLLLoss()\n",
    "\n",
    "losses = []\n",
    "for sample in range(100):\n",
    "    print(sample)\n",
    "    optim.zero_grad()\n",
    "\n",
    "    input, labels = dataset[sample]\n",
    "    loss = loss_func(model(input), labels)\n",
    "    losses.append(loss)\n",
    "\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text(model, vocab, chars, context_length):\n",
    "    outputs = []\n",
    "    for i in range(chars):\n",
    "        if i == 0:\n",
    "            input = [torch.zeros((1, len(vocab)))]\n",
    "        elif i < context_length:\n",
    "            # Add one here because the : is exclusive on the right\n",
    "            input = outputs[: i + 1]\n",
    "        else:\n",
    "            input = outputs[i - context_length : i]\n",
    "        \n",
    "        idx = model(pack_sequence(input)).argmax()\n",
    "        output = torch.zeros((1, len(vocab)))\n",
    "        output[:, idx] = 1\n",
    "        outputs.append(output)\n",
    "\n",
    "    return torch.vstack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                                                    '"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detokenize(vocab, gen_text(model, vocab, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
